# -*- coding: utf-8 -*-
"""paris-renting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TqGKwThML9B1lSVMgq8Tbs4FQOA8ls60
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os

import seaborn as sns
# %matplotlib inline

os.chdir("drive/colab-notebooks/paris-renting")

data = pd.read_json("seloger.json", orient = "records")

def type_encoder(x):
  return {'Studio': 0,"Appartement F3":1, "Appartement F4":2}[x]
train = data.copy()
train['type'] = train['type'].apply(type_encoder)

train.head().append(train.tail())

train.nunique()

from pylab import rcParams
rcParams['figure.figsize'] = 14, 10

sns.countplot(x='type',hue = 'code_post', data=data)

train['type'].value_counts()

train.describe()

train["price"].plot(kind='hist',bins=11,xlim=(300,1400))

data_zs = (train-train.mean())/train.std()

data_zs.describe()

from sklearn.cluster import KMeans
model = KMeans(n_clusters = 2, max_iter = 25) #分为k类
#model = KMeans(n_clusters = k, n_jobs = 4, max_iter = iteration) #分为k类，并发数4
model.fit(data_zs) #开始聚类

#简单打印结果
r1 = pd.Series(model.labels_).value_counts() #统计各个类别的数目
r2 = pd.DataFrame(model.cluster_centers_) #找出聚类中心
r = pd.concat([r2, r1], axis = 1) #横向连接（0是纵向），得到聚类中心对应的类别下的数目
print(r)
r.columns = list(data.columns) + [u'nb_cluster'] #重命名表头
print(r)

#详细输出原始数据及其类别
r = pd.concat([data, pd.Series(model.labels_, index = data.index)], axis = 1)  #详细输出每个样本对应的类别
r.columns = list(data.columns) + [u'cluster'] #重命名表头

### Silhouette Coefficient

r['cluster'].value_counts()

from sklearn.manifold import TSNE
tsne = TSNE(random_state = 64)
tsne.fit_transform(data_zs) #进行数据降维,并返回结果
tsne = pd.DataFrame(tsne.embedding_, index = data_zs.index) #转换数据格式

import matplotlib.pyplot as plt


#不同类别用不同颜色和样式绘图
d = tsne[r[u'cluster'] == 0]     #找出聚类类别为0的数据对应的降维结果
plt.plot(d[0], d[1], 'r.')
d = tsne[r[u'cluster'] == 1]
plt.plot(d[0], d[1], 'go')
d = tsne[r[u'cluster'] == 2]
plt.plot(d[0], d[1], 'b*')
d = tsne[r[u'cluster'] == 3]
plt.plot(d[0], d[1], '*', color = 'black')
plt.show()

data.groupby('type').count()

from sklearn.manifold import TSNE
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

class data():
    def __init__(self, data, target):
        self.data = data
        self.target = target

# 加载数据集
iris = load_iris()

data_zs.describe()

from sklearn.manifold import TSNE
tsne = TSNE(random_state =66)
tsne.fit_transform(data_zs)
tsne = pd.DataFrame(tsne.embedding_, index = data_zs.index)

plt.figure(figsize = (14,8))

plt.scatter(tsne[0],tsne[1])

tsne_zs = (tsne-tsne.mean())/tsne.std()

from sklearn.cluster import KMeans
model = KMeans(n_clusters = 2, max_iter = 20 ) #k clusters
model.fit(tsne_zs)

r1 = pd.Series(model.labels_).value_counts() #统计各个类别的数目
r2 = pd.DataFrame(model.cluster_centers_) #找出聚类中心
r = pd.concat([r2, r1], axis = 1) #横向连接（0是纵向），得到聚类中心对应的类别下的数目
print(r)
r.columns = list([1,2]) + [u'nb_cluster'] #重命名表头
print(r)
r = pd.concat([tsne, pd.Series(model.labels_, index = tsne.index)], axis = 1)  #详细输出每个样本对应的类别
r.columns = list(tsne.columns)+[u'Cluster']

plt.figure(figsize = (14,8))
d = tsne[r[u'Cluster'] == 0]
plt.plot(d[0], d[1], 'r.')
d = tsne[r[u'Cluster'] == 1]
plt.plot(d[0], d[1], 'go')
d = tsne[r[u'Cluster'] == 2]
plt.plot(d[0], d[1], 'b*')
d = tsne[r[u'Cluster'] == 3]
plt.plot(d[0], d[1], '*', color = 'black')
plt.show()

data_clustered = pd.concat([r.Cluster,data],axis =1)

data_clustered.head()

sns.countplot(x = "Cluster", hue = "type", data = data_clustered)

prizes = data_clustered.groupby("Cluster")["price"].mean()

sizes = data_clustered.groupby("Cluster")["size"].mean()

prizes / sizes

prizes

sizes

import math
from sklearn.linear_model import LinearRegression
class LWLR(object):
  """
  Locally weighted Linear Regression
  """
    def __init__(self, k):
        self.k = k
    
    def fit(self,X,y):
        self.X = X
        self.y = y
        return self
        
        
    def predict(self,X):
        result = []
        for i in range(pd.DataFrame(X).shape[0]):
            prediction = self.predict_single(pd.DataFrame(X).iloc[i])
            result.append(prediction)
        print(result)
        return np.array(result)
        
    def _predict_single(self,x):
        X = self.X
        y = self.y
        alpha = 0.1
        n_features = X.shape[1]
        beta = np.array([0.0]*n_features)
        n_rounds = 100
        W = (-1/(2*self.k)*(((self.X-np.array(x))**2).sum(axis = 1))).apply(lambda x: math.exp(x))
        print(W)
        for i in range(n_rounds):

            #Calculate epsilon
            epsilon = self.y
            for j in range(n_features):
                epsilon = epsilon - beta[j]*X.iloc[:,j]

            #Update beta
            for j in range(n_features):
                gradient = -np.mean(W*epsilon*X.iloc[:,j])
                beta[j] = beta[j] - alpha*gradient
        result = (beta * x).sum()
        return result
      
    def predict_single(self,example):
        example = np.array(example)
        weights = np.square(self.X-example)
        weights = np.sum(weights,axis=1)
        weights = -0.5*weights/np.square(self.k)
        weights = np.exp(weights)
        model = LinearRegression()
        model.fit(X=self.X,y=self.y,sample_weight=weights)
        return model.predict(np.reshape(example,[1,-1]))[0]

lwlr = LWLR(1)

lwlr.fit(x_train,y_train)

x_train = train.loc[:,["size","code_post","type"]]

y_train = train.price

lwlr.predict(np.array([20,75016,0]).reshape(1,3))

train[train['size']==0]

