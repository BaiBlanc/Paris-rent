# -*- coding: utf-8 -*-
"""paris-renting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TqGKwThML9B1lSVMgq8Tbs4FQOA8ls60
"""

"""
Authentification for files in google Drive

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import seaborn as sns
# %matplotlib inline

os.chdir("drive/colab-notebooks/paris-renting")

# Read the paris renting dataset
data = pd.read_json("seloger.json", orient = "records")

# Encoding categorical features
def type_encoder(x):
  return {'Studio': 0,"Appartement F3":1, "Appartement F4":2}[x]
train = data.copy()
train['type'] = train['type'].apply(type_encoder)

# EDA
train.head().append(train.tail())
train.nunique()
train['type'].value_counts()
train.describe()
from pylab import rcParams
rcParams['figure.figsize'] = 14, 10
sns.countplot(x='type',hue = 'code_post', data=data)
train["price"].plot(kind='hist',bins=11,xlim=(300,1400))

# Z-score data for K-means
data_zs = (train-train.mean())/train.std()
data_zs.describe()

from sklearn.cluster import KMeans
model = KMeans(n_clusters = 2, max_iter = 25) # 2 clusters
model.fit(data_zs) #Star clustering

# Print the results
r1 = pd.Series(model.labels_).value_counts() #Number of data in each cluster
r2 = pd.DataFrame(model.cluster_centers_) #Find the center
r = pd.concat([r2, r1], axis = 1)
print(r)
r.columns = list(data.columns) + [u'nb_cluster'] #Rename the table
print(r)

# Add the information od cluster into origin dataset
r = pd.concat([data, pd.Series(model.labels_, index = data.index)], axis = 1)
r.columns = list(data.columns) + [u'cluster']

### Silhouette Coefficient
r['cluster'].value_counts()

# Reduction of dimension with t-SNE
from sklearn.manifold import TSNE
tsne = TSNE(random_state = 64)
tsne.fit_transform(data_zs)
tsne = pd.DataFrame(tsne.embedding_, index = data_zs.index) #Change the data tyoe

import matplotlib.pyplot as plt

# Visualize the dataset with reduced dimensionlity
# Choose different color for different cluster (4 color predefined)
d = tsne[r[u'cluster'] == 0]
plt.plot(d[0], d[1], 'r.')
d = tsne[r[u'cluster'] == 1]
plt.plot(d[0], d[1], 'go')
d = tsne[r[u'cluster'] == 2]
plt.plot(d[0], d[1], 'b*')
d = tsne[r[u'cluster'] == 3]
plt.plot(d[0], d[1], '*', color = 'black')
plt.show()

data.groupby('type').count()

# EDA with clustered data
data_clustered = pd.concat([r.cluster,data],axis =1)
data_clustered.head()

sns.countplot(x = "Cluster", hue = "type", data = data_clustered)

prizes = data_clustered.groupby("Cluster")["price"].mean()

sizes = data_clustered.groupby("Cluster")["size"].mean()

prizes / sizes

prizes

sizes

# Prediction of price with LWLR which is realised manually
import math
from sklearn.linear_model import LinearRegression
class LWLR(object):
  """
  Locally weighted Linear Regression
  LWLR is a lazy learning model,
  which means the calculation will be done only when receive the test data
  """
    def __init__(self, k):
        self.k = k
    
    def fit(self,X,y):
        """
        Only stock the training set without any calculation
        :param X: Training-set
        :param y: Labels
        :return: self
        """
        self.X = X
        self.y = y
        return self
        
        
    def predict(self,X):
        """
        Call the mecthod @predict_single() to predict singly
        :param X: The training set
        :return: The nparray of results
        """
        result = []
        for i in range(pd.DataFrame(X).shape[0]):
            prediction = self.predict_single(pd.DataFrame(X).iloc[i])
            result.append(prediction)
        print(result)
        return np.array(result)
        
    def _predict_single(self,x):
        X = self.X
        y = self.y
        alpha = 0.1
        n_features = X.shape[1]
        beta = np.array([0.0]*n_features)
        n_rounds = 100
        W = (-1/(2*self.k)*(((self.X-np.array(x))**2).sum(axis = 1))).apply(lambda x: math.exp(x))
        print(W)
        for i in range(n_rounds):

            #Calculate epsilon
            epsilon = self.y
            for j in range(n_features):
                epsilon = epsilon - beta[j]*X.iloc[:,j]

            #Update beta
            for j in range(n_features):
                gradient = -np.mean(W*epsilon*X.iloc[:,j])
                beta[j] = beta[j] - alpha*gradient
        result = (beta * x).sum()
        return result
      
    def predict_single(self,example):
        """
        Calculate the weights considering the distance with each training data
        :param example: A single data for prediction
        :return: The single result of prediction
        """
        example = np.array(example)
        weights = np.square(self.X-example)
        weights = np.sum(weights,axis=1)
        weights = -0.5*weights/np.square(self.k)
        weights = np.exp(weights)
        model = LinearRegression()
        model.fit(X=self.X,y=self.y,sample_weight=weights)
        return model.predict(np.reshape(example,[1,-1]))[0]

lwlr = LWLR(1)

x_train = train.loc[:,["size","code_post","type"]]
y_train = train.price
lwlr.fit(x_train,y_train)


# Try to predict a 20 m2 studio in the 16th arrondisment (borough)
lwlr.predict(np.array([20,75016,0]).reshape(1,3))
# Result: 858.9711624335032
# 859 euro/month Sigh...


